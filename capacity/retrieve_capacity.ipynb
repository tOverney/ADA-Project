{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "import requests\n",
    "import time as ti\n",
    "import pandas as pd\n",
    "import time\n",
    "from itertools import count\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ditch_stop =['132','133','134','135','136','137','138', '139', '140', '141','142','174','175', '176']\n",
    "\n",
    "def get_json_backend(date, page):\n",
    "    url = 'http://192.168.99.100/api/trip/?agency=31&agency=45&agency=52&service=True&date=' + date\n",
    "    results = []\n",
    "    print(\"Retrieving backend trips for\", date)\n",
    "    first = True\n",
    "    while True:\n",
    "\n",
    "        r = requests.get(url)\n",
    "        j = r.json()\n",
    "        \n",
    "        if first:\n",
    "            print(j['count'])\n",
    "            first = False\n",
    "        \n",
    "        #print(r.url)\n",
    "        results = results + j['results']\n",
    "        url = j['next']\n",
    "        if not url:\n",
    "            break\n",
    "        \n",
    "        page = page - 1\n",
    "        if page == 0:\n",
    "            break\n",
    "            \n",
    "    return results\n",
    "\n",
    "def get_request_transport(params):\n",
    "    base_url=\"http://transport.opendata.ch/v1/connections\"\n",
    "    r = requests.get(base_url, params=params)\n",
    "    return r\n",
    "\n",
    "def print_back(trip, f, t ):\n",
    "    print(trip['start'], trip['count'], f, t, trip['headsign'])\n",
    "    for stop in trip['stops']:\n",
    "        print(stop['stop_id'], stop['stop_sequence'], stop['id'], stop['stop_name'])\n",
    "        \n",
    "def get_info_backend(trip):\n",
    "    stop_dict = {stop['stop_id']: stop for stop in trip['stops'] if stop['stop_id'] not in ditch_stop}\n",
    "    f = next((item['stop_id'] for item in trip['stops'] if item['stop_sequence'] == 0), None)\n",
    "    t = next((item['stop_id'] for item in trip['stops'] if item['stop_sequence'] == trip['count']-1), None)    \n",
    "    return stop_dict, f, t\n",
    "\n",
    "def parse_time(t):\n",
    "    try:\n",
    "        return str(parse(t).time())\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "def process_trip(trip, date, time):\n",
    "    stop_backend, f, t = get_info_backend(trip)\n",
    "\n",
    "    params = {\n",
    "        'from': f,\n",
    "        'to': t,\n",
    "        'date': date,\n",
    "        'time': time,\n",
    "        'direct':1,\n",
    "        'limit':1\n",
    "    }\n",
    "    \n",
    "    trans_r = get_request_transport(params)\n",
    "    \n",
    "    try:\n",
    "        sections_transport = get_info_transport(trans_r.json()['connections'])\n",
    "    except Exception as e:\n",
    "        print(trans_r.url)\n",
    "        print(trans_r.json())\n",
    "        raise ValueError(\"No connections\") \n",
    "        \n",
    "    for journey in sections_transport:\n",
    "        # Verify we go through all stops\n",
    "        #TODO should verify if start at the same time.\n",
    "        if stop_backend.keys() == journey.keys(): \n",
    "            return trans_r.json()\n",
    "    print()\n",
    "    print(\"Error ##########\")\n",
    "    \n",
    "    print(trans_r.url)\n",
    "    print_back(trip, f, t)\n",
    "    print(\"-  -  -\")\n",
    "    for journey in sections_transport:\n",
    "        for k, stop in journey.items():\n",
    "            print(stop['station']['id'], stop['station']['name'])\n",
    "    \n",
    "    raise ValueError(\"Stops mismatch\") \n",
    "    \n",
    "def buzzergen(period):\n",
    "    nexttime = time.time() + period\n",
    "    for i in count():\n",
    "        now = time.time()\n",
    "        tosleep = nexttime - now\n",
    "        if tosleep > 0:\n",
    "            time.sleep(tosleep)\n",
    "            nexttime += period\n",
    "        else:\n",
    "            nexttime = now + period\n",
    "        yield i, nexttime\n",
    "\n",
    "\n",
    "\n",
    "def get_info_transport(connections): \n",
    "    passList = []\n",
    "    for connection in connections:\n",
    "        journey_stops = {}\n",
    "        for section in connection['sections']:\n",
    "            if section['journey']:\n",
    "                journey_stops.update({stop['station']['id']: stop for stop in section['journey']['passList']})\n",
    "                \n",
    "        passList.append(journey_stops)\n",
    "    \n",
    "    return passList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buzzer = buzzergen(0.4)\n",
    "directory = 'data/'\n",
    "def get_capacity_data(date, page=1000, batch=1000):\n",
    "    back_list = get_json_backend(date, page=page)\n",
    "    print(\"Processing\", len(back_list))\n",
    "    \n",
    "    day_dict = []\n",
    "    error = []\n",
    "    for i, trip in enumerate(back_list):  \n",
    "        if i % batch == 0 and i != 0: \n",
    "            print(\"Saving *********\", i)\n",
    "            pickle.dump(day_dict, open(directory + 'occupancy_' + date + '.pkl', 'wb'))\n",
    "        \n",
    "        # Wait at least 0.5 seconds between each call to the transport API\n",
    "        next(buzzer)\n",
    "        \n",
    "        try:\n",
    "            transport_json = process_trip(trip, date, trip['start'])  \n",
    "            \n",
    "            out = {\n",
    "                'date': date, \n",
    "                'backend': trip,\n",
    "                'transport': transport_json,\n",
    "            }\n",
    "            \n",
    "            day_dict.append(out)\n",
    "            \n",
    "        except ValueError as e:\n",
    "            print(\"=>\", e)\n",
    "            error.append(trip)\n",
    "        except Exception as e:\n",
    "            print(\"=>\",e)\n",
    "            error.append(trip)\n",
    "            \n",
    "        ti.sleep(0.4)\n",
    "    \n",
    "    pickle.dump(day_dict, open(directory + 'occupancy_' + date + '.pkl', 'wb'))\n",
    "    pickle.dump(error, open(directory + 'error_' + date + '.pkl', 'wb'))\n",
    "    return error, day_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dates = [\"2017-01-09\", \"2017-01-10\", \"2017-01-11\", \"2017-01-13\", \"2017-01-14\", \"2017-01-15\", \"2017-01-16\"]\n",
    "\n",
    "for date in dates:\n",
    "    get_capacity_data(date, page=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
